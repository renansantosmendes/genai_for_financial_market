# -*- coding: utf-8 -*-
"""Untitled150.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zZy6SZAwh94FH45oHr3YJJMebY7_Qnqh
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
import yfinance as yf
from datetime import datetime, timedelta
import warnings
import random
import os
from genai_for_financial_market.vae.visualization import (
    plot_training_curves,
    compare_distributions,
    create_candlestick_chart,
    plot_candlestick_grid,
    plot_price_evolution,
    plot_volume_analysis,
    plot_technical_indicators,
    plot_return_analysis,
)

warnings.filterwarnings('ignore')

def set_seed(seed=42):
    """Configure seeds for complete reproducibility."""
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

SEED = 42
set_seed(SEED)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class FinancialVAE(nn.Module):
    """Variational Autoencoder for financial time series data."""

    def __init__(self, input_dim, latent_dim=32, hidden_dims=[128, 64]):
        super(FinancialVAE, self).__init__()
        self.input_dim = input_dim
        self.latent_dim = latent_dim

        encoder_layers = []
        prev_dim = input_dim
        for h_dim in hidden_dims:
            encoder_layers.extend([
                nn.Linear(prev_dim, h_dim),
                nn.ReLU(),
                nn.BatchNorm1d(h_dim),
                nn.Dropout(0.2)
            ])
            prev_dim = h_dim
        self.encoder = nn.Sequential(*encoder_layers)
        self.fc_mu = nn.Linear(hidden_dims[-1], latent_dim)
        self.fc_var = nn.Linear(hidden_dims[-1], latent_dim)

        decoder_layers = []
        prev_dim = latent_dim
        for h_dim in reversed(hidden_dims):
            decoder_layers.extend([
                nn.Linear(prev_dim, h_dim),
                nn.ReLU(),
                nn.BatchNorm1d(h_dim),
                nn.Dropout(0.2)
            ])
            prev_dim = h_dim
        decoder_layers.append(nn.Linear(prev_dim, input_dim))
        self.decoder = nn.Sequential(*decoder_layers)

    def encode(self, x):
        h = self.encoder(x)
        return self.fc_mu(h), self.fc_var(h)

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        generator = torch.Generator(device=mu.device).manual_seed(SEED)
        eps = torch.randn(std.size(), device=mu.device, generator=generator)
        return mu + eps * std

    def decode(self, z):
        return self.decoder(z)

    def forward(self, x):
        mu, logvar = self.encode(x)
        z = self.reparameterize(mu, logvar)
        return self.decode(z), mu, logvar

def vae_loss_function(recon_x, x, mu, logvar, beta=1.0):
    """VAE loss combining reconstruction and KL regularization."""
    recon_loss = F.mse_loss(recon_x, x, reduction='mean')
    kld_loss = torch.mean(-0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1), dim=0)
    return recon_loss + beta * kld_loss, recon_loss, kld_loss

def fetch_financial_data(symbols, period='2y'):
    """Fetch financial data using yfinance."""
    data = {}
    for symbol in symbols:
        try:
            ticker = yf.Ticker(symbol)
            df = ticker.history(period=period)
            if not df.empty:
                data[symbol] = df
                print(f"✓ Data collected for {symbol}: {len(df)} records")
            else:
                print(f"✗ No data found for {symbol}")
        except Exception as e:
            print(f"✗ Error collecting {symbol}: {e}")
    return data

def prepare_financial_features(df):
    """Prepare financial features including technical indicators."""
    features = df[['Open', 'High', 'Low', 'Close', 'Volume']].copy()
    features['Return'] = df['Close'].pct_change()
    features['Log_Return'] = np.log(df['Close'] / df['Close'].shift(1))
    features['Volatility'] = features['Return'].rolling(window=20).std()
    features['SMA_10'] = df['Close'].rolling(window=10).mean()
    features['SMA_30'] = df['Close'].rolling(window=30).mean()
    features['EMA_12'] = df['Close'].ewm(span=12).mean()

    delta = df['Close'].diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
    rs = gain / loss
    features['RSI'] = 100 - (100 / (1 + rs))

    exp1 = df['Close'].ewm(span=12).mean()
    exp2 = df['Close'].ewm(span=26).mean()
    features['MACD'] = exp1 - exp2
    features['MACD_Signal'] = features['MACD'].ewm(span=9).mean()

    sma_20 = df['Close'].rolling(window=20).mean()
    std_20 = df['Close'].rolling(window=20).std()
    features['BB_Upper'] = sma_20 + (std_20 * 2)
    features['BB_Lower'] = sma_20 - (std_20 * 2)
    features['BB_Width'] = features['BB_Upper'] - features['BB_Lower']

    return features.dropna()

def create_sequences(data, seq_length=20, seed=None):
    """Create temporal sequences for training."""
    if seed is not None:
        np.random.seed(seed)

    sequences = []
    targets = []
    for i in range(len(data) - seq_length):
        seq = data[i:(i + seq_length)]
        target = data[i + seq_length]
        sequences.append(seq.flatten())
        targets.append(target)

    return np.array(sequences), np.array(targets)

def train_vae(model, train_loader, num_epochs=100, lr=1e-3, beta=1.0, seed=None):
    """Train the VAE model."""
    if seed is not None:
        torch.manual_seed(seed)
        np.random.seed(seed)

    optimizer = optim.Adam(model.parameters(), lr=lr)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, factor=0.5)

    model.train()
    losses = []
    recon_losses = []
    kld_losses = []

    for epoch in range(num_epochs):
        total_loss = 0
        total_recon = 0
        total_kld = 0

        for batch_idx, data in enumerate(train_loader):
            data = data.to(device)
            optimizer.zero_grad()

            recon_batch, mu, logvar = model(data)
            loss, recon_loss, kld_loss = vae_loss_function(recon_batch, data, mu, logvar, beta)

            loss.backward()
            optimizer.step()

            total_loss += loss.item()
            total_recon += recon_loss.item()
            total_kld += kld_loss.item()

        avg_loss = total_loss / len(train_loader)
        avg_recon = total_recon / len(train_loader)
        avg_kld = total_kld / len(train_loader)

        losses.append(avg_loss)
        recon_losses.append(avg_recon)
        kld_losses.append(avg_kld)

        scheduler.step(avg_loss)

        if epoch % 10 == 0:
            print(f'Epoch {epoch:3d} | Loss: {avg_loss:.4f} | Recon: {avg_recon:.4f} | KLD: {avg_kld:.4f}')

    return losses, recon_losses, kld_losses

def generate_synthetic_data(model, num_samples, feature_names, scaler, seq_length=20, seed=None):
    """Generate synthetic data using the trained model."""
    if seed is not None:
        torch.manual_seed(seed)

    model.eval()
    with torch.no_grad():
        generator = torch.Generator(device=device).manual_seed(seed if seed else SEED)
        z = torch.randn(num_samples, model.latent_dim, generator=generator).to(device)
        synthetic_flat = model.decode(z).cpu().numpy()

        n_features = len(feature_names)
        synthetic_sequences = synthetic_flat.reshape(-1, seq_length, n_features)

        synthetic_data = []
        for seq in synthetic_sequences:
            seq_denorm = scaler.inverse_transform(seq)
            synthetic_data.append(seq_denorm)

        return np.array(synthetic_data)

def load_trained_model(model_path='financial_vae_model.pth'):
    """Load a trained VAE model."""
    try:
        checkpoint = torch.load(model_path, map_location=device, weights_only=False)

        input_dim = None
        for key, value in checkpoint['model_state_dict'].items():
            if 'encoder.0.weight' in key:
                input_dim = value.shape[1]
                break

        if input_dim is None:
            raise ValueError("Could not determine input_dim from model")

        model = FinancialVAE(
            input_dim=input_dim,
            latent_dim=checkpoint.get('latent_dim', 32)
        ).to(device)

        model.load_state_dict(checkpoint['model_state_dict'])
        model.eval()

        return model, checkpoint

    except FileNotFoundError:
        print("❌ Model file not found!")
        print("Run the training script first to create 'financial_vae_model.pth'")
        return None, None

def create_synthetic_dataframes(synthetic_sequences, feature_names, seq_length):
    """Convert synthetic sequences to structured DataFrames."""
    synthetic_dfs = []

    for i, seq in enumerate(synthetic_sequences):
        start_date = datetime(2024, 1, 1) + timedelta(days=i*30)
        dates = pd.date_range(start=start_date, periods=seq_length, freq='D')

        df = pd.DataFrame(seq, columns=feature_names, index=dates)
        df.name = f'Synthetic_Series_{i+1}'
        synthetic_dfs.append(df)

    return synthetic_dfs

def create_market_summary_dashboard(synthetic_dfs):
    """Create synthetic market summary dashboard."""
    n_series = len(synthetic_dfs)

    print("=" * 60)
    print("📊 DASHBOARD - GENERATED SYNTHETIC MARKET")
    print("=" * 60)

    total_periods = sum(len(df) for df in synthetic_dfs)
    avg_price = np.mean([df['Close'].mean() for df in synthetic_dfs])
    avg_volume = np.mean([df['Volume'].mean() for df in synthetic_dfs])
    avg_volatility = np.mean([df['Return'].std() for df in synthetic_dfs])

    print(f"🔢 GENERAL STATISTICS:")
    print(f"   • Number of generated series: {n_series}")
    print(f"   • Total periods: {total_periods:,}")
    print(f"   • Average price: ${avg_price:.2f}")
    print(f"   • Average volume: {avg_volume:,.0f}")
    print(f"   • Average volatility: {avg_volatility:.4f}")

    all_returns = []
    for df in synthetic_dfs:
        all_returns.extend(df['Return'].dropna().tolist())

    print(f"\n📈 RETURN ANALYSIS:")
    print(f"   • Average return: {np.mean(all_returns):.4f}")
    print(f"   • Overall volatility: {np.std(all_returns):.4f}")
    print(f"   • Minimum return: {np.min(all_returns):.4f}")
    print(f"   • Maximum return: {np.max(all_returns):.4f}")
    print(f"   • Skewness: {pd.Series(all_returns).skew():.4f}")
    print(f"   • Kurtosis: {pd.Series(all_returns).kurtosis():.4f}")

    avg_rsi = np.mean([df['RSI'].mean() for df in synthetic_dfs if 'RSI' in df.columns])
    print(f"\n🔧 TECHNICAL INDICATORS:")
    print(f"   • Average RSI: {avg_rsi:.2f}")

    nan_count = sum(df.isnull().sum().sum() for df in synthetic_dfs)
    print(f"\n✅ DATA QUALITY:")
    print(f"   • NaN values: {nan_count}")
    print(f"   • Data integrity: {'✓' if nan_count == 0 else '✗'}")

    print("=" * 60)

def train_main():
    """Main function for training the VAE model."""
    symbols = ['AAPL', 'GOOGL', 'MSFT', 'TSLA', 'NVDA']
    seq_length = 20
    latent_dim = 32
    batch_size = 64
    num_epochs = 50

    print("=== Starting Financial Market VAE System ===")
    print(f"🌱 Running with seed: {SEED} (reproducible)\n")

    print("1. Collecting financial data...")
    financial_data = fetch_financial_data(symbols, period='2y')

    if not financial_data:
        print("Error: No financial data collected!")
        return

    print("\n2. Preparing financial features...")
    all_features = []

    for symbol, df in financial_data.items():
        features = prepare_financial_features(df)
        all_features.append(features.values)
        print(f"✓ Features prepared for {symbol}: {features.shape}")

    combined_data = np.vstack(all_features)
    feature_names = list(prepare_financial_features(list(financial_data.values())[0]).columns)

    print(f"✓ Combined dataset: {combined_data.shape}")
    print(f"✓ Features: {feature_names}")

    print("\n3. Normalizing data...")
    scaler = StandardScaler()
    normalized_data = scaler.fit_transform(combined_data)

    print("\n4. Creating temporal sequences...")
    sequences, _ = create_sequences(normalized_data, seq_length)
    print(f"✓ Sequences created: {sequences.shape}")

    def seed_worker(worker_id):
        worker_seed = torch.initial_seed() % 2**32
        np.random.seed(worker_seed)
        random.seed(worker_seed)

    g = torch.Generator()
    g.manual_seed(SEED)

    train_data = torch.FloatTensor(sequences)
    train_loader = torch.utils.data.DataLoader(
        train_data,
        batch_size=batch_size,
        shuffle=True,
        worker_init_fn=seed_worker,
        generator=g
    )

    print(f"\n5. Creating VAE model...")
    input_dim = sequences.shape[1]
    model = FinancialVAE(input_dim, latent_dim).to(device)

    print(f"✓ Model created - Input: {input_dim}, Latent: {latent_dim}")
    print(f"✓ Model parameters: {sum(p.numel() for p in model.parameters()):,}")

    print(f"\n6. Training model...")
    losses, recon_losses, kld_losses = train_vae(
        model, train_loader, num_epochs=num_epochs, lr=1e-3, beta=1.0, seed=SEED
    )

    print(f"\n7. Generating synthetic data...")
    num_synthetic_samples = 100
    synthetic_sequences = generate_synthetic_data(
        model, num_synthetic_samples, feature_names, scaler, seq_length, seed=SEED
    )

    print(f"✓ Synthetic data generated: {synthetic_sequences.shape}")

    print(f"\n8. Creating visualizations...")
    plot_training_curves(losses, recon_losses, kld_losses)

    real_sequences = normalized_data[:num_synthetic_samples*seq_length].reshape(num_synthetic_samples, seq_length, -1)
    real_sequences_denorm = np.array([scaler.inverse_transform(seq) for seq in real_sequences])

    compare_distributions(real_sequences_denorm, synthetic_sequences, feature_names)

    print("\n9. Creating candlestick charts...")
    real_sample = real_sequences_denorm[0]
    fig_real = create_candlestick_chart(real_sample[:, :4], "Real Data - OHLC")
    fig_real.show()

    synthetic_sample = synthetic_sequences[0]
    fig_synthetic = create_candlestick_chart(synthetic_sample[:, :4], "Synthetic Data - OHLC")
    fig_synthetic.show()

    print("\n=== Evaluation Metrics ===")
    for i, feature in enumerate(feature_names[:5]):
        real_mean = np.mean(real_sequences_denorm[:, :, i])
        synthetic_mean = np.mean(synthetic_sequences[:, :, i])
        real_std = np.std(real_sequences_denorm[:, :, i])
        synthetic_std = np.std(synthetic_sequences[:, :, i])

        print(f"{feature}:")
        print(f"  Mean - Real: {real_mean:.4f}, Synthetic: {synthetic_mean:.4f}")
        print(f"  Std - Real: {real_std:.4f}, Synthetic: {synthetic_std:.4f}")

    print("\n=== VAE System Completed ===")
    print(f"✓ Model trained with {len(sequences)} sequences")
    print(f"✓ {num_synthetic_samples} synthetic sequences generated")
    print(f"✓ Data ready for analysis and strategy testing")
    print(f"🌱 Reproducible execution with seed: {SEED}")

    torch.save({
        'model_state_dict': model.state_dict(),
        'scaler': scaler,
        'feature_names': feature_names,
        'seq_length': seq_length,
        'latent_dim': latent_dim,
        'seed': SEED
    }, 'financial_vae_model.pth')
    print("✓ Model saved as 'financial_vae_model.pth'")

def generate_main():
    """Main function for generating synthetic data."""
    print("🚀 Synthetic Data Generator - Financial VAE")
    print("=" * 50)

    print("📂 Loading trained model...")
    model, checkpoint = load_trained_model()

    if model is None:
        return

    print("✓ Model loaded successfully!")
    print(f"✓ Latent dimension: {checkpoint.get('latent_dim', 32)}")
    print(f"✓ Sequence length: {checkpoint.get('seq_length', 20)}")

    scaler = checkpoint['scaler']
    feature_names = checkpoint['feature_names']
    seq_length = checkpoint['seq_length']

    print(f"\n🎲 Generating synthetic series...")
    num_samples = 50

    synthetic_sequences = generate_synthetic_data(
        model=model,
        num_samples=num_samples,
        feature_names=feature_names,
        scaler=scaler,
        seq_length=seq_length,
        seed=SEED
    )

    print(f"✓ {len(synthetic_sequences)} series generated!")
    print(f"✓ Each series has {seq_length} periods")
    print(f"✓ Features per period: {len(feature_names)}")

    print(f"\n📊 Structuring data...")
    synthetic_dfs = create_synthetic_dataframes(
        synthetic_sequences, feature_names, seq_length
    )

    print(f"✓ DataFrames created with timestamps")

    create_market_summary_dashboard(synthetic_dfs)

    print(f"\n📈 Generating visualizations...")

    print("   • Candlestick grid...")
    fig_candlestick = plot_candlestick_grid(synthetic_dfs, n_plots=6)
    fig_candlestick.show()

    print("   • Price evolution...")
    fig_prices = plot_price_evolution(synthetic_dfs, n_series=10)
    fig_prices.show()

    print("   • Volume analysis...")
    plot_volume_analysis(synthetic_dfs, n_series=8)

    print("   • Technical indicators...")
    fig_technical = plot_technical_indicators(synthetic_dfs, series_idx=0)
    fig_technical.show()

    print("   • Return analysis...")
    plot_return_analysis(synthetic_dfs, n_series=10)

    # export_data = input("\n💾 Export synthetic data? (y/n):\n")


if __name__ == "__main__":
    train_main()
    generate_main()

