# -*- coding: utf-8 -*-
"""Untitled150.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zZy6SZAwh94FH45oHr3YJJMebY7_Qnqh
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
import yfinance as yf
from datetime import datetime, timedelta
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import plotly.express as px
import warnings
import random
import os
from scipy import stats
from statsmodels.tsa.stattools import acf

warnings.filterwarnings('ignore')

def set_seed(seed=42):
    """Configure seeds for complete reproducibility."""
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

SEED = 42
set_seed(SEED)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class FinancialVAE(nn.Module):
    """Variational Autoencoder for financial time series data."""

    def __init__(self, input_dim, latent_dim=32, hidden_dims=[128, 64]):
        super(FinancialVAE, self).__init__()
        self.input_dim = input_dim
        self.latent_dim = latent_dim

        encoder_layers = []
        prev_dim = input_dim
        for h_dim in hidden_dims:
            encoder_layers.extend([
                nn.Linear(prev_dim, h_dim),
                nn.ReLU(),
                nn.BatchNorm1d(h_dim),
                nn.Dropout(0.2)
            ])
            prev_dim = h_dim
        self.encoder = nn.Sequential(*encoder_layers)
        self.fc_mu = nn.Linear(hidden_dims[-1], latent_dim)
        self.fc_var = nn.Linear(hidden_dims[-1], latent_dim)

        decoder_layers = []
        prev_dim = latent_dim
        for h_dim in reversed(hidden_dims):
            decoder_layers.extend([
                nn.Linear(prev_dim, h_dim),
                nn.ReLU(),
                nn.BatchNorm1d(h_dim),
                nn.Dropout(0.2)
            ])
            prev_dim = h_dim
        decoder_layers.append(nn.Linear(prev_dim, input_dim))
        self.decoder = nn.Sequential(*decoder_layers)

    def encode(self, x):
        h = self.encoder(x)
        return self.fc_mu(h), self.fc_var(h)

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        generator = torch.Generator(device=mu.device).manual_seed(SEED)
        eps = torch.randn(std.size(), device=mu.device, generator=generator)
        return mu + eps * std

    def decode(self, z):
        return self.decoder(z)

    def forward(self, x):
        mu, logvar = self.encode(x)
        z = self.reparameterize(mu, logvar)
        return self.decode(z), mu, logvar

def vae_loss_function(recon_x, x, mu, logvar, beta=1.0):
    """VAE loss combining reconstruction and KL regularization."""
    recon_loss = F.mse_loss(recon_x, x, reduction='mean')
    kld_loss = torch.mean(-0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1), dim=0)
    return recon_loss + beta * kld_loss, recon_loss, kld_loss

def fetch_financial_data(symbols, period='2y'):
    """Fetch financial data using yfinance."""
    data = {}
    for symbol in symbols:
        try:
            ticker = yf.Ticker(symbol)
            df = ticker.history(period=period)
            if not df.empty:
                data[symbol] = df
                print(f"âœ“ Data collected for {symbol}: {len(df)} records")
            else:
                print(f"âœ— No data found for {symbol}")
        except Exception as e:
            print(f"âœ— Error collecting {symbol}: {e}")
    return data

def prepare_financial_features(df):
    """Prepare financial features including technical indicators."""
    features = df[['Open', 'High', 'Low', 'Close', 'Volume']].copy()
    features['Return'] = df['Close'].pct_change()
    features['Log_Return'] = np.log(df['Close'] / df['Close'].shift(1))
    features['Volatility'] = features['Return'].rolling(window=20).std()
    features['SMA_10'] = df['Close'].rolling(window=10).mean()
    features['SMA_30'] = df['Close'].rolling(window=30).mean()
    features['EMA_12'] = df['Close'].ewm(span=12).mean()

    delta = df['Close'].diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
    rs = gain / loss
    features['RSI'] = 100 - (100 / (1 + rs))

    exp1 = df['Close'].ewm(span=12).mean()
    exp2 = df['Close'].ewm(span=26).mean()
    features['MACD'] = exp1 - exp2
    features['MACD_Signal'] = features['MACD'].ewm(span=9).mean()

    sma_20 = df['Close'].rolling(window=20).mean()
    std_20 = df['Close'].rolling(window=20).std()
    features['BB_Upper'] = sma_20 + (std_20 * 2)
    features['BB_Lower'] = sma_20 - (std_20 * 2)
    features['BB_Width'] = features['BB_Upper'] - features['BB_Lower']

    return features.dropna()

def create_sequences(data, seq_length=20, seed=None):
    """Create temporal sequences for training."""
    if seed is not None:
        np.random.seed(seed)

    sequences = []
    targets = []
    for i in range(len(data) - seq_length):
        seq = data[i:(i + seq_length)]
        target = data[i + seq_length]
        sequences.append(seq.flatten())
        targets.append(target)

    return np.array(sequences), np.array(targets)

def train_vae(model, train_loader, num_epochs=100, lr=1e-3, beta=1.0, seed=None):
    """Train the VAE model."""
    if seed is not None:
        torch.manual_seed(seed)
        np.random.seed(seed)

    optimizer = optim.Adam(model.parameters(), lr=lr)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, factor=0.5)

    model.train()
    losses = []
    recon_losses = []
    kld_losses = []

    for epoch in range(num_epochs):
        total_loss = 0
        total_recon = 0
        total_kld = 0

        for batch_idx, data in enumerate(train_loader):
            data = data.to(device)
            optimizer.zero_grad()

            recon_batch, mu, logvar = model(data)
            loss, recon_loss, kld_loss = vae_loss_function(recon_batch, data, mu, logvar, beta)

            loss.backward()
            optimizer.step()

            total_loss += loss.item()
            total_recon += recon_loss.item()
            total_kld += kld_loss.item()

        avg_loss = total_loss / len(train_loader)
        avg_recon = total_recon / len(train_loader)
        avg_kld = total_kld / len(train_loader)

        losses.append(avg_loss)
        recon_losses.append(avg_recon)
        kld_losses.append(avg_kld)

        scheduler.step(avg_loss)

        if epoch % 10 == 0:
            print(f'Epoch {epoch:3d} | Loss: {avg_loss:.4f} | Recon: {avg_recon:.4f} | KLD: {avg_kld:.4f}')

    return losses, recon_losses, kld_losses

def generate_synthetic_data(model, num_samples, feature_names, scaler, seq_length=20, seed=None):
    """Generate synthetic data using the trained model."""
    if seed is not None:
        torch.manual_seed(seed)

    model.eval()
    with torch.no_grad():
        generator = torch.Generator(device=device).manual_seed(seed if seed else SEED)
        z = torch.randn(num_samples, model.latent_dim, generator=generator).to(device)
        synthetic_flat = model.decode(z).cpu().numpy()

        n_features = len(feature_names)
        synthetic_sequences = synthetic_flat.reshape(-1, seq_length, n_features)

        synthetic_data = []
        for seq in synthetic_sequences:
            seq_denorm = scaler.inverse_transform(seq)
            synthetic_data.append(seq_denorm)

        return np.array(synthetic_data)

def load_trained_model(model_path='financial_vae_model.pth'):
    """Load a trained VAE model."""
    try:
        checkpoint = torch.load(model_path, map_location=device, weights_only=False)

        input_dim = None
        for key, value in checkpoint['model_state_dict'].items():
            if 'encoder.0.weight' in key:
                input_dim = value.shape[1]
                break

        if input_dim is None:
            raise ValueError("Could not determine input_dim from model")

        model = FinancialVAE(
            input_dim=input_dim,
            latent_dim=checkpoint.get('latent_dim', 32)
        ).to(device)

        model.load_state_dict(checkpoint['model_state_dict'])
        model.eval()

        return model, checkpoint

    except FileNotFoundError:
        print("âŒ Model file not found!")
        print("Run the training script first to create 'financial_vae_model.pth'")
        return None, None

def create_synthetic_dataframes(synthetic_sequences, feature_names, seq_length):
    """Convert synthetic sequences to structured DataFrames."""
    synthetic_dfs = []

    for i, seq in enumerate(synthetic_sequences):
        start_date = datetime(2024, 1, 1) + timedelta(days=i*30)
        dates = pd.date_range(start=start_date, periods=seq_length, freq='D')

        df = pd.DataFrame(seq, columns=feature_names, index=dates)
        df.name = f'Synthetic_Series_{i+1}'
        synthetic_dfs.append(df)

    return synthetic_dfs

def plot_training_curves(losses, recon_losses, kld_losses):
    """Plot training curves."""
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))

    axes[0, 0].plot(losses)
    axes[0, 0].set_title('Total Loss')
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].set_ylabel('Loss')
    axes[0, 0].grid(True)

    axes[0, 1].plot(recon_losses, color='orange')
    axes[0, 1].set_title('Reconstruction Loss')
    axes[0, 1].set_xlabel('Epoch')
    axes[0, 1].set_ylabel('MSE Loss')
    axes[0, 1].grid(True)

    axes[1, 0].plot(kld_losses, color='red')
    axes[1, 0].set_title('KL Divergence')
    axes[1, 0].set_xlabel('Epoch')
    axes[1, 0].set_ylabel('KLD')
    axes[1, 0].grid(True)

    axes[1, 1].plot(recon_losses, label='Reconstruction', color='orange')
    axes[1, 1].plot(kld_losses, label='KL Divergence', color='red')
    axes[1, 1].set_title('Loss Comparison')
    axes[1, 1].set_xlabel('Epoch')
    axes[1, 1].set_ylabel('Loss')
    axes[1, 1].legend()
    axes[1, 1].grid(True)

    plt.tight_layout()
    plt.show()

def compare_distributions(real_data, synthetic_data, feature_names):
    """Compare distributions of real vs synthetic data."""
    n_features = min(4, len(feature_names))
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    axes = axes.flatten()

    for i in range(n_features):
        if i < len(feature_names):
            real_flat = real_data[:, :, i].flatten()
            synthetic_flat = synthetic_data[:, :, i].flatten()

            axes[i].hist(real_flat, alpha=0.5, label='Real', density=True, bins=50)
            axes[i].hist(synthetic_flat, alpha=0.5, label='Synthetic', density=True, bins=50)
            axes[i].set_title(f'Distribution: {feature_names[i]}')
            axes[i].legend()
            axes[i].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

def create_candlestick_chart(data, title="Candlestick Data"):
    """Create candlestick chart using plotly."""
    fig = go.Figure(data=go.Candlestick(
        x=list(range(len(data))),
        open=data[:, 0],
        high=data[:, 1],
        low=data[:, 2],
        close=data[:, 3],
        name=title
    ))

    fig.update_layout(
        title=title,
        yaxis_title='Price',
        xaxis_title='Time',
        template='plotly_white'
    )

    return fig

def plot_candlestick_grid(synthetic_dfs, n_plots=6, title="Synthetic Series - Candlestick"):
    """Create grid of candlestick charts."""
    n_plots = min(n_plots, len(synthetic_dfs))

    fig = make_subplots(
        rows=2, cols=3,
        subplot_titles=[f'Series {i+1}' for i in range(n_plots)],
        vertical_spacing=0.08,
        horizontal_spacing=0.05
    )

    for i in range(n_plots):
        row = (i // 3) + 1
        col = (i % 3) + 1

        df = synthetic_dfs[i]

        fig.add_trace(
            go.Candlestick(
                x=df.index,
                open=df['Open'],
                high=df['High'],
                low=df['Low'],
                close=df['Close'],
                name=f'Series {i+1}',
                showlegend=False
            ),
            row=row, col=col
        )

    fig.update_layout(
        title=title,
        height=800,
        template='plotly_white'
    )

    return fig

def plot_price_evolution(synthetic_dfs, n_series=10):
    """Plot closing price evolution."""
    fig = go.Figure()
    colors = px.colors.qualitative.Set3

    for i, df in enumerate(synthetic_dfs[:n_series]):
        fig.add_trace(go.Scatter(
            x=df.index,
            y=df['Close'],
            mode='lines',
            name=f'Series {i+1}',
            line=dict(color=colors[i % len(colors)], width=1.5),
            opacity=0.8
        ))

    fig.update_layout(
        title=f'Closing Price Evolution - {n_series} Synthetic Series',
        xaxis_title='Date',
        yaxis_title='Closing Price',
        template='plotly_white',
        height=600
    )

    return fig

def plot_volume_analysis(synthetic_dfs, n_series=8):
    """Analyze volume characteristics of synthetic series."""
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))

    all_volumes = []
    daily_avg_volumes = []

    for df in synthetic_dfs[:n_series]:
        all_volumes.extend(df['Volume'].tolist())
        daily_avg_volumes.append(df['Volume'].mean())

    axes[0, 0].hist(all_volumes, bins=50, alpha=0.7, color='skyblue', edgecolor='black')
    axes[0, 0].set_title('Volume Distribution - All Series')
    axes[0, 0].set_xlabel('Volume')
    axes[0, 0].set_ylabel('Frequency')
    axes[0, 0].grid(True, alpha=0.3)

    axes[0, 1].bar(range(1, len(daily_avg_volumes)+1), daily_avg_volumes,
                   color='lightcoral', alpha=0.8)
    axes[0, 1].set_title('Average Volume per Synthetic Series')
    axes[0, 1].set_xlabel('Series')
    axes[0, 1].set_ylabel('Average Volume')
    axes[0, 1].grid(True, alpha=0.3)

    price_vol_corr = []
    for df in synthetic_dfs[:n_series]:
        corr = df['Close'].corr(df['Volume'])
        price_vol_corr.append(corr)

    axes[1, 0].bar(range(1, len(price_vol_corr)+1), price_vol_corr,
                   color='gold', alpha=0.8)
    axes[1, 0].set_title('Price-Volume Correlation per Series')
    axes[1, 0].set_xlabel('Series')
    axes[1, 0].set_ylabel('Correlation')
    axes[1, 0].axhline(y=0, color='red', linestyle='--', alpha=0.5)
    axes[1, 0].grid(True, alpha=0.3)

    volume_data = [df['Volume'].values for df in synthetic_dfs[:n_series]]
    axes[1, 1].boxplot(volume_data, labels=[f'S{i+1}' for i in range(len(volume_data))])
    axes[1, 1].set_title('Volume Distribution by Series')
    axes[1, 1].set_xlabel('Series')
    axes[1, 1].set_ylabel('Volume')
    axes[1, 1].grid(True, alpha=0.3)

    plt.tight_layout()
    return fig

def plot_technical_indicators(synthetic_dfs, series_idx=0):
    """Plot technical indicators for a specific series."""
    df = synthetic_dfs[series_idx]

    fig = make_subplots(
        rows=3, cols=2,
        subplot_titles=[
            'Price and Moving Averages', 'RSI',
            'MACD', 'Bollinger Bands',
            'Volatility', 'Volume'
        ],
        vertical_spacing=0.08,
        specs=[[{"secondary_y": False}, {"secondary_y": False}],
               [{"secondary_y": False}, {"secondary_y": False}],
               [{"secondary_y": False}, {"secondary_y": False}]]
    )

    fig.add_trace(go.Scatter(x=df.index, y=df['Close'], name='Price', line=dict(color='blue')), row=1, col=1)
    fig.add_trace(go.Scatter(x=df.index, y=df['SMA_10'], name='SMA 10', line=dict(color='red')), row=1, col=1)
    fig.add_trace(go.Scatter(x=df.index, y=df['SMA_30'], name='SMA 30', line=dict(color='green')), row=1, col=1)

    fig.add_trace(go.Scatter(x=df.index, y=df['RSI'], name='RSI', line=dict(color='purple')), row=1, col=2)
    fig.add_hline(y=70, line_dash="dash", line_color="red", row=1, col=2)
    fig.add_hline(y=30, line_dash="dash", line_color="green", row=1, col=2)

    fig.add_trace(go.Scatter(x=df.index, y=df['MACD'], name='MACD', line=dict(color='blue')), row=2, col=1)
    fig.add_trace(go.Scatter(x=df.index, y=df['MACD_Signal'], name='Signal', line=dict(color='red')), row=2, col=1)

    fig.add_trace(go.Scatter(x=df.index, y=df['Close'], name='Price', line=dict(color='blue')), row=2, col=2)
    fig.add_trace(go.Scatter(x=df.index, y=df['BB_Upper'], name='BB Upper', line=dict(color='red')), row=2, col=2)
    fig.add_trace(go.Scatter(x=df.index, y=df['BB_Lower'], name='BB Lower', line=dict(color='green')), row=2, col=2)

    fig.add_trace(go.Scatter(x=df.index, y=df['Volatility'], name='Volatility', line=dict(color='orange')), row=3, col=1)

    fig.add_trace(go.Bar(x=df.index, y=df['Volume'], name='Volume', marker_color='lightblue'), row=3, col=2)

    fig.update_layout(
        title=f'Technical Indicators - Synthetic Series {series_idx + 1}',
        height=1000,
        template='plotly_white',
        showlegend=False
    )

    return fig

def plot_return_analysis(synthetic_dfs, n_series=10):
    """Detailed return analysis of synthetic series."""
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))

    all_returns = []
    all_log_returns = []

    for df in synthetic_dfs[:n_series]:
        all_returns.extend(df['Return'].dropna().tolist())
        all_log_returns.extend(df['Log_Return'].dropna().tolist())

    axes[0, 0].hist(all_returns, bins=50, alpha=0.7, color='lightblue', edgecolor='black', density=True)
    axes[0, 0].set_title('Return Distribution')
    axes[0, 0].set_xlabel('Return')
    axes[0, 0].set_ylabel('Density')
    axes[0, 0].axvline(x=0, color='red', linestyle='--', alpha=0.5)
    axes[0, 0].grid(True, alpha=0.3)

    stats.probplot(all_returns, dist="norm", plot=axes[0, 1])
    axes[0, 1].set_title('Q-Q Plot - Return Normality')
    axes[0, 1].grid(True, alpha=0.3)

    volatilities = [df['Return'].std() for df in synthetic_dfs[:n_series]]
    axes[0, 2].bar(range(1, len(volatilities)+1), volatilities, color='coral', alpha=0.8)
    axes[0, 2].set_title('Volatility per Series')
    axes[0, 2].set_xlabel('Series')
    axes[0, 2].set_ylabel('Volatility (Std)')
    axes[0, 2].grid(True, alpha=0.3)

    for i, df in enumerate(synthetic_dfs[:5]):
        cumret = (1 + df['Return'].fillna(0)).cumprod() - 1
        axes[1, 0].plot(cumret.index, cumret.values, label=f'Series {i+1}', alpha=0.8)
    axes[1, 0].set_title('Cumulative Returns')
    axes[1, 0].set_xlabel('Date')
    axes[1, 0].set_ylabel('Cumulative Return')
    axes[1, 0].legend()
    axes[1, 0].grid(True, alpha=0.3)

    if len(all_returns) > 50:
        autocorr = acf(all_returns, nlags=20, fft=True)
        axes[1, 1].bar(range(len(autocorr)), autocorr, color='green', alpha=0.7)
        axes[1, 1].set_title('Return Autocorrelation')
        axes[1, 1].set_xlabel('Lag')
        axes[1, 1].set_ylabel('Autocorrelation')
        axes[1, 1].grid(True, alpha=0.3)

    sharpe_ratios = [df['Return'].mean() / df['Return'].std() if df['Return'].std() > 0 else 0
                     for df in synthetic_dfs[:n_series]]
    axes[1, 2].bar(range(1, len(sharpe_ratios)+1), sharpe_ratios, color='purple', alpha=0.8)
    axes[1, 2].set_title('Sharpe Ratio per Series')
    axes[1, 2].set_xlabel('Series')
    axes[1, 2].set_ylabel('Sharpe Ratio')
    axes[1, 2].axhline(y=0, color='red', linestyle='--', alpha=0.5)
    axes[1, 2].grid(True, alpha=0.3)

    plt.tight_layout()
    return fig

def create_market_summary_dashboard(synthetic_dfs):
    """Create synthetic market summary dashboard."""
    n_series = len(synthetic_dfs)

    print("=" * 60)
    print("ðŸ“Š DASHBOARD - GENERATED SYNTHETIC MARKET")
    print("=" * 60)

    total_periods = sum(len(df) for df in synthetic_dfs)
    avg_price = np.mean([df['Close'].mean() for df in synthetic_dfs])
    avg_volume = np.mean([df['Volume'].mean() for df in synthetic_dfs])
    avg_volatility = np.mean([df['Return'].std() for df in synthetic_dfs])

    print(f"ðŸ”¢ GENERAL STATISTICS:")
    print(f"   â€¢ Number of generated series: {n_series}")
    print(f"   â€¢ Total periods: {total_periods:,}")
    print(f"   â€¢ Average price: ${avg_price:.2f}")
    print(f"   â€¢ Average volume: {avg_volume:,.0f}")
    print(f"   â€¢ Average volatility: {avg_volatility:.4f}")

    all_returns = []
    for df in synthetic_dfs:
        all_returns.extend(df['Return'].dropna().tolist())

    print(f"\nðŸ“ˆ RETURN ANALYSIS:")
    print(f"   â€¢ Average return: {np.mean(all_returns):.4f}")
    print(f"   â€¢ Overall volatility: {np.std(all_returns):.4f}")
    print(f"   â€¢ Minimum return: {np.min(all_returns):.4f}")
    print(f"   â€¢ Maximum return: {np.max(all_returns):.4f}")
    print(f"   â€¢ Skewness: {pd.Series(all_returns).skew():.4f}")
    print(f"   â€¢ Kurtosis: {pd.Series(all_returns).kurtosis():.4f}")

    avg_rsi = np.mean([df['RSI'].mean() for df in synthetic_dfs if 'RSI' in df.columns])
    print(f"\nðŸ”§ TECHNICAL INDICATORS:")
    print(f"   â€¢ Average RSI: {avg_rsi:.2f}")

    nan_count = sum(df.isnull().sum().sum() for df in synthetic_dfs)
    print(f"\nâœ… DATA QUALITY:")
    print(f"   â€¢ NaN values: {nan_count}")
    print(f"   â€¢ Data integrity: {'âœ“' if nan_count == 0 else 'âœ—'}")

    print("=" * 60)

def train_main():
    """Main function for training the VAE model."""
    symbols = ['AAPL', 'GOOGL', 'MSFT', 'TSLA', 'NVDA']
    seq_length = 20
    latent_dim = 32
    batch_size = 64
    num_epochs = 50

    print("=== Starting Financial Market VAE System ===")
    print(f"ðŸŒ± Running with seed: {SEED} (reproducible)\n")

    print("1. Collecting financial data...")
    financial_data = fetch_financial_data(symbols, period='2y')

    if not financial_data:
        print("Error: No financial data collected!")
        return

    print("\n2. Preparing financial features...")
    all_features = []

    for symbol, df in financial_data.items():
        features = prepare_financial_features(df)
        all_features.append(features.values)
        print(f"âœ“ Features prepared for {symbol}: {features.shape}")

    combined_data = np.vstack(all_features)
    feature_names = list(prepare_financial_features(list(financial_data.values())[0]).columns)

    print(f"âœ“ Combined dataset: {combined_data.shape}")
    print(f"âœ“ Features: {feature_names}")

    print("\n3. Normalizing data...")
    scaler = StandardScaler()
    normalized_data = scaler.fit_transform(combined_data)

    print("\n4. Creating temporal sequences...")
    sequences, _ = create_sequences(normalized_data, seq_length)
    print(f"âœ“ Sequences created: {sequences.shape}")

    def seed_worker(worker_id):
        worker_seed = torch.initial_seed() % 2**32
        np.random.seed(worker_seed)
        random.seed(worker_seed)

    g = torch.Generator()
    g.manual_seed(SEED)

    train_data = torch.FloatTensor(sequences)
    train_loader = torch.utils.data.DataLoader(
        train_data,
        batch_size=batch_size,
        shuffle=True,
        worker_init_fn=seed_worker,
        generator=g
    )

    print(f"\n5. Creating VAE model...")
    input_dim = sequences.shape[1]
    model = FinancialVAE(input_dim, latent_dim).to(device)

    print(f"âœ“ Model created - Input: {input_dim}, Latent: {latent_dim}")
    print(f"âœ“ Model parameters: {sum(p.numel() for p in model.parameters()):,}")

    print(f"\n6. Training model...")
    losses, recon_losses, kld_losses = train_vae(
        model, train_loader, num_epochs=num_epochs, lr=1e-3, beta=1.0, seed=SEED
    )

    print(f"\n7. Generating synthetic data...")
    num_synthetic_samples = 100
    synthetic_sequences = generate_synthetic_data(
        model, num_synthetic_samples, feature_names, scaler, seq_length, seed=SEED
    )

    print(f"âœ“ Synthetic data generated: {synthetic_sequences.shape}")

    print(f"\n8. Creating visualizations...")
    plot_training_curves(losses, recon_losses, kld_losses)

    real_sequences = normalized_data[:num_synthetic_samples*seq_length].reshape(num_synthetic_samples, seq_length, -1)
    real_sequences_denorm = np.array([scaler.inverse_transform(seq) for seq in real_sequences])

    compare_distributions(real_sequences_denorm, synthetic_sequences, feature_names)

    print("\n9. Creating candlestick charts...")
    real_sample = real_sequences_denorm[0]
    fig_real = create_candlestick_chart(real_sample[:, :4], "Real Data - OHLC")
    fig_real.show()

    synthetic_sample = synthetic_sequences[0]
    fig_synthetic = create_candlestick_chart(synthetic_sample[:, :4], "Synthetic Data - OHLC")
    fig_synthetic.show()

    print("\n=== Evaluation Metrics ===")
    for i, feature in enumerate(feature_names[:5]):
        real_mean = np.mean(real_sequences_denorm[:, :, i])
        synthetic_mean = np.mean(synthetic_sequences[:, :, i])
        real_std = np.std(real_sequences_denorm[:, :, i])
        synthetic_std = np.std(synthetic_sequences[:, :, i])

        print(f"{feature}:")
        print(f"  Mean - Real: {real_mean:.4f}, Synthetic: {synthetic_mean:.4f}")
        print(f"  Std - Real: {real_std:.4f}, Synthetic: {synthetic_std:.4f}")

    print("\n=== VAE System Completed ===")
    print(f"âœ“ Model trained with {len(sequences)} sequences")
    print(f"âœ“ {num_synthetic_samples} synthetic sequences generated")
    print(f"âœ“ Data ready for analysis and strategy testing")
    print(f"ðŸŒ± Reproducible execution with seed: {SEED}")

    torch.save({
        'model_state_dict': model.state_dict(),
        'scaler': scaler,
        'feature_names': feature_names,
        'seq_length': seq_length,
        'latent_dim': latent_dim,
        'seed': SEED
    }, 'financial_vae_model.pth')
    print("âœ“ Model saved as 'financial_vae_model.pth'")

def generate_main():
    """Main function for generating synthetic data."""
    print("ðŸš€ Synthetic Data Generator - Financial VAE")
    print("=" * 50)

    print("ðŸ“‚ Loading trained model...")
    model, checkpoint = load_trained_model()

    if model is None:
        return

    print("âœ“ Model loaded successfully!")
    print(f"âœ“ Latent dimension: {checkpoint.get('latent_dim', 32)}")
    print(f"âœ“ Sequence length: {checkpoint.get('seq_length', 20)}")

    scaler = checkpoint['scaler']
    feature_names = checkpoint['feature_names']
    seq_length = checkpoint['seq_length']

    print(f"\nðŸŽ² Generating synthetic series...")
    num_samples = 50

    synthetic_sequences = generate_synthetic_data(
        model=model,
        num_samples=num_samples,
        feature_names=feature_names,
        scaler=scaler,
        seq_length=seq_length,
        seed=SEED
    )

    print(f"âœ“ {len(synthetic_sequences)} series generated!")
    print(f"âœ“ Each series has {seq_length} periods")
    print(f"âœ“ Features per period: {len(feature_names)}")

    print(f"\nðŸ“Š Structuring data...")
    synthetic_dfs = create_synthetic_dataframes(
        synthetic_sequences, feature_names, seq_length
    )

    print(f"âœ“ DataFrames created with timestamps")

    create_market_summary_dashboard(synthetic_dfs)

    print(f"\nðŸ“ˆ Generating visualizations...")

    print("   â€¢ Candlestick grid...")
    fig_candlestick = plot_candlestick_grid(synthetic_dfs, n_plots=6)
    fig_candlestick.show()

    print("   â€¢ Price evolution...")
    fig_prices = plot_price_evolution(synthetic_dfs, n_series=10)
    fig_prices.show()

    print("   â€¢ Volume analysis...")
    fig_volume = plot_volume_analysis(synthetic_dfs, n_series=8)
    plt.show()

    print("   â€¢ Technical indicators...")
    fig_technical = plot_technical_indicators(synthetic_dfs, series_idx=0)
    fig_technical.show()

    print("   â€¢ Return analysis...")
    fig_returns = plot_return_analysis(synthetic_dfs, n_series=10)
    plt.show()

    # export_data = input("\nðŸ’¾ Export synthetic data? (y/n):

if __name__ == "__main__":
    train_main()
    generate_main()

